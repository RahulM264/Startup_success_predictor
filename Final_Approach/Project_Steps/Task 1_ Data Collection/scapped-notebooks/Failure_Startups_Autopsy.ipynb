{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Failure_Startups_Autopsy.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il-CujvH_sFa"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import Request, urlopen\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itLX3xPCABUS"
      },
      "source": [
        "site_base_url = 'https://getautopsy.wixsite.com/failed-startups/allcompanies/'\n",
        "companies = ['Amiloom', 'Backfence', 'Circa-News', 'DeviceFidelity', 'Gumroad-', 'Homejoy', 'Juicero', 'Kichit', 'Lookery', 'Netscape', 'Next-step-living', 'OpTier', 'Path', 'Poliana', 'Shyp', 'Skully', 'Thrively', 'Tutorspree', 'Zulily', 'Yik-Yak', 'eCrowds']\n",
        "N = len(companies)\n",
        "us_comp_links = [site_base_url + c for c in companies]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYiLQW8PeCoo"
      },
      "source": [
        "### Utility Function to scrape the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD9nasxmC5Sv"
      },
      "source": [
        "def get_requests(url):\n",
        "  \n",
        "  \"\"\"\n",
        "  Get the status of the request to the url\n",
        "  \"\"\"\n",
        "\n",
        "  time.sleep(1)\n",
        "  \n",
        "  hdr = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36\"}\n",
        "  \n",
        "  req = Request(url,headers=hdr)\n",
        "  try:\n",
        "    page = urlopen(req)\n",
        "    soup = BeautifulSoup(page)\n",
        "    return soup\n",
        "  \n",
        "  except Exception as exc:\n",
        "    print(f\"The problem is {exc}\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVACnb22eCaX"
      },
      "source": [
        "def get_dataheaders_from_links(url):\n",
        "\n",
        "  \"\"\"\n",
        "  Extract the data from the given company link\n",
        "  \"\"\"\n",
        "\n",
        "  soup = get_requests(url)  \n",
        "  header_ = soup.find_all('p',attrs={'class':'font_8'})\n",
        "\n",
        "  headerlist = []\n",
        "  for i in header_:\n",
        "    headerlist.append('_'.join(i.text.split(' ')))\n",
        "\n",
        "  company_name = headerlist[0]\n",
        "\n",
        "  subheader_ = soup.find_all('h5',attrs={'class':'font_5'})\n",
        "\n",
        "  subheaderlist = []\n",
        "  for i in subheader_:\n",
        "    subheaderlist.append('_'.join(i.text.split(' ')))\n",
        "  \n",
        "  header_indices = [1,8,10,12,14,18,21,25,27,29,31,33,37,39,41,43,45,47,49]\n",
        "\n",
        "  data_headers = [subheaderlist[i] for i in header_indices]\n",
        "\n",
        "  data_headers.insert(0,'Company_Name')\n",
        "  data_headers.insert(2,'Company_type')\n",
        "  data_headers.insert(3,'Business_type')\n",
        "  data_headers.insert(4,'Description')\n",
        "   \n",
        "  return data_headers,company_name,subheaderlist"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p-N_Rvja2PR"
      },
      "source": [
        "def get_data(company_name,subheaderlist):\n",
        "  \n",
        "  \"\"\"\n",
        "  Fetch the data and create a list \n",
        "  \"\"\"\n",
        "\n",
        "  data_indices = [2,3,4,5,9,11,13,15,19,22,26,28,30,32,34,38,40,42,44,46,48,50]\n",
        "  \n",
        "  insert_data_list = [subheaderlist[i] for i in data_indices] \n",
        "  insert_data_list.insert(0,company_name)\n",
        "  \n",
        "  return insert_data_list"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TM8CN6MrYj3A"
      },
      "source": [
        "def main():\n",
        "\n",
        "  \"\"\"\n",
        "  Main Function to execute\n",
        "  \"\"\"\n",
        "\n",
        "  companies_data_list = []\n",
        "\n",
        "  for link in tqdm(us_comp_links):\n",
        "    \n",
        "    data_headers,company_name,subheaderlist = get_dataheaders_from_links(link)\n",
        "    temp_list = get_data(company_name,subheaderlist)\n",
        "    companies_data_list.append(temp_list)\n",
        "\n",
        "  data_df = pd.DataFrame(columns = data_headers,index = np.arange(0,N))\n",
        "\n",
        "  for i in range(N):\n",
        "    \n",
        "    data_df.loc[i] = companies_data_list[i]  \n",
        "    data_df = data_df.sort_index() \n",
        "  \n",
        "  data_df.to_csv('failure_startup_autopsy.csv')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7G4PGHxYj0M"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}